{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-22T10:33:18.462917Z",
     "start_time": "2026-02-22T10:33:12.410767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "#from Course.PyTorch_Intro.PyTorch_intro import vocab_size\n",
    "\n",
    "# Hyperparameters\n",
    "window_size = 100\n",
    "seq_length = 99\n",
    "embed_size = 128\n",
    "hidden_size = 256\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "id": "4847e5e70204467c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-22T10:33:32.857827Z",
     "start_time": "2026-02-22T10:33:32.845236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data preparation\n",
    "def get_data():\n",
    "    print(\"Text downloading...\")\n",
    "    url = \"https://www.gutenberg.org/cache/epub/84/pg84-images.html\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    words = words[:50000]\n",
    "    vocab = sorted(list(set(words)))\n",
    "    word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "    ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "    print(f\"Words count՝ {len(words)}\")\n",
    "    print(f\"Unique words (Vocab size)՝ {len(vocab)}\")\n",
    "\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for i in range(len(words) - window_size + 1):\n",
    "        input_seq = [word_to_ix[w] for w in words[i : i + seq_length]]\n",
    "        target_word = word_to_ix[words[i + seq_length]]\n",
    "\n",
    "        inputs.append(input_seq)\n",
    "        targets.append(target_word)\n",
    "\n",
    "    X = torch.LongTensor(inputs)\n",
    "    y = torch.LongTensor(targets)\n",
    "\n",
    "    return X, y, len(vocab), word_to_ix, ix_to_word\n",
    "\n",
    "class TextGenRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(TextGenRNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x) # (Batch_Size, 99, Embed_Size)\n",
    "        out, hidden = self.rnn(embeds)\n",
    "        last_output = out[:, -1, :]\n",
    "        prediction = self.fc(last_output)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "def generate_text(model, start_words, length, word_to_ix, ix_to_word):\n",
    "    model.eval()\n",
    "\n",
    "    current_words = start_words.split()\n",
    "    input_seq = [word_to_ix.get(w, 0) for w in current_words]\n",
    "\n",
    "    if len(input_seq) < seq_length:\n",
    "        input_seq = [0] * (seq_length - len(input_seq)) + input_seq\n",
    "\n",
    "    generated_text = list(current_words)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            x = torch.LongTensor([input_seq[-seq_length:]]).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(x)\n",
    "\n",
    "            probs = torch.softmax(output, dim=1).cpu().numpy()[0]\n",
    "\n",
    "\n",
    "            next_ix = np.random.choice(len(probs), p=probs)\n",
    "            next_word = ix_to_word[next_ix]\n",
    "\n",
    "            generated_text.append(next_word)\n",
    "            input_seq.append(next_ix)\n",
    "\n",
    "    return \" \".join(generated_text)"
   ],
   "id": "587aef1d97dd174a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-22T10:53:09.895412Z",
     "start_time": "2026-02-22T10:33:33.692676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    X, y, vocab_size, word_to_ix, ix_to_word = get_data()\n",
    "    dataset = TensorDataset(X, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    model = TextGenRNN(vocab_size, embed_size, hidden_size).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # training loop\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_inputs, batch_targets in dataloader:\n",
    "            batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(batch_inputs)\n",
    "            # Loss Calculation\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "\n",
    "            # Backward & Step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        sample_seed = \"i saw the lightning playing on the summit of mont blanc\"\n",
    "        print(f\"Sample: {generate_text(model, sample_seed, 10, word_to_ix, ix_to_word)}...\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "    # Final Generation\n",
    "    print(\"\\n--- Final Training Results ---\")\n",
    "    seed_sentence = \"the creature opened his dull yellow eye and looked at me with a grin\"\n",
    "    final_text = generate_text(model, seed_sentence, 100, word_to_ix, ix_to_word)\n",
    "    print(final_text)"
   ],
   "id": "b6d46fd74b2e1010",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text downloading...\n",
      "Words count՝ 50000\n",
      "Unique words (Vocab size)՝ 5873\n",
      "Starting training...\n",
      "Epoch 1/20, Loss: 6.3886\n",
      "Sample: i saw the lightning playing on the summit of mont blanc by of evil brought he unfortunate the question might loose...\n",
      "------------------------------\n",
      "Epoch 2/20, Loss: 5.4749\n",
      "Sample: i saw the lightning playing on the summit of mont blanc or the bid you and only and he diffused skirting...\n",
      "------------------------------\n",
      "Epoch 3/20, Loss: 4.8217\n",
      "Sample: i saw the lightning playing on the summit of mont blanc with fury break our cares with delight bread within me...\n",
      "------------------------------\n",
      "Epoch 4/20, Loss: 4.1979\n",
      "Sample: i saw the lightning playing on the summit of mont blanc me seated mine they were every one else food when...\n",
      "------------------------------\n",
      "Epoch 5/20, Loss: 3.6121\n",
      "Sample: i saw the lightning playing on the summit of mont blanc as i spoke in a quick air that he said...\n",
      "------------------------------\n",
      "Epoch 6/20, Loss: 3.0877\n",
      "Sample: i saw the lightning playing on the summit of mont blanc i feel that i have hastened with the greatest eagerness...\n",
      "------------------------------\n",
      "Epoch 7/20, Loss: 2.6392\n",
      "Sample: i saw the lightning playing on the summit of mont blanc towards him you the innocent of a paroxysm and console...\n",
      "------------------------------\n",
      "Epoch 8/20, Loss: 2.2601\n",
      "Sample: i saw the lightning playing on the summit of mont blanc in as the pleasant appearance of cousin for familiar intercourse...\n",
      "------------------------------\n",
      "Epoch 9/20, Loss: 1.9412\n",
      "Sample: i saw the lightning playing on the summit of mont blanc were finished a passage affected that she played some man...\n",
      "------------------------------\n",
      "Epoch 10/20, Loss: 1.6698\n",
      "Sample: i saw the lightning playing on the summit of mont blanc to give the language like a manner and i enjoy...\n",
      "------------------------------\n",
      "Epoch 11/20, Loss: 1.4410\n",
      "Sample: i saw the lightning playing on the summit of mont blanc in turkey and i saw of some thick underwood determining...\n",
      "------------------------------\n",
      "Epoch 12/20, Loss: 1.2454\n",
      "Sample: i saw the lightning playing on the summit of mont blanc in that the vegetables of my story in the flesh...\n",
      "------------------------------\n",
      "Epoch 13/20, Loss: 1.0781\n",
      "Sample: i saw the lightning playing on the summit of mont blanc in the idea of the conclusions i had melt my...\n",
      "------------------------------\n",
      "Epoch 14/20, Loss: 0.9390\n",
      "Sample: i saw the lightning playing on the summit of mont blanc in preceding entered but when he has retired to a...\n",
      "------------------------------\n",
      "Epoch 15/20, Loss: 0.8148\n",
      "Sample: i saw the lightning playing on the summit of mont blanc in my bosom s eyes was full by his pleasure...\n",
      "------------------------------\n",
      "Epoch 16/20, Loss: 0.7134\n",
      "Sample: i saw the lightning playing on the summit of mont blanc in the most of the people of cold on which...\n",
      "------------------------------\n",
      "Epoch 17/20, Loss: 0.6294\n",
      "Sample: i saw the lightning playing on the summit of mont blanc in the loveliness of youth into my sweet arabian i...\n",
      "------------------------------\n",
      "Epoch 18/20, Loss: 0.5461\n",
      "Sample: i saw the lightning playing on the summit of mont blanc the supreme world to light into my voice more town...\n",
      "------------------------------\n",
      "Epoch 19/20, Loss: 0.4869\n",
      "Sample: i saw the lightning playing on the summit of mont blanc the supreme which was at the feet of elizabeth that...\n",
      "------------------------------\n",
      "Epoch 20/20, Loss: 0.4384\n",
      "Sample: i saw the lightning playing on the summit of mont blanc in torture for his amiable and in the meantime took...\n",
      "------------------------------\n",
      "\n",
      "--- Final Training Results ---\n",
      "the creature opened his dull yellow eye and looked at me with a grin wrinkled the wood and soon and down on the desired of despair but every other but this not soon arrived on the lake with such a good family in france youth a great of which i could drink more conveniently than his countenance but the sun was not one of the murderer on the door that all every country we met made up but he received a wish to destroy all who had directed their inquiries as i most of her for many years of age we thought could have a very ill he tormented poor yet that could me\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
